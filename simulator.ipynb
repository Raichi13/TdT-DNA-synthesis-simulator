{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SqTs5cznfBEO"
      },
      "source": [
        "# バイナリから塩基配列に変換\n",
        "\n",
        "処理としてはECCを付与してから塩基配列に変換\\\n",
        "分割する場合、合成すべき配列のリストを返す\n",
        "\n",
        "## 論点\n",
        "\n",
        "\n",
        "*   巨大なデータの場合、分割するのか\n",
        "*   分割する場合、アドレスのサイズ\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8rBhX-kFByFI",
        "outputId": "58bba29d-5236-4a7f-b51b-b2503ac3f017"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: bitarray in /usr/local/lib/python3.7/dist-packages (2.6.0)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (1.21.6)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: hamming-codec in /usr/local/lib/python3.7/dist-packages (0.3.5)\n",
            "Requirement already satisfied: typer in /usr/local/lib/python3.7/dist-packages (from hamming-codec) (0.4.2)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.7/dist-packages (from typer->hamming-codec) (7.1.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install bitarray\n",
        "!pip install numpy\n",
        "!pip install hamming-codec"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 255,
      "metadata": {},
      "outputs": [],
      "source": [
        "from bitarray import bitarray\n",
        "import numpy as np\n",
        "import hamming_codec\n",
        "import itertools\n",
        "import matplotlib.pyplot as plt\n",
        "import random\n",
        "import pandas as pd\n",
        "import json\n",
        "import time, datetime\n",
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 228,
      "metadata": {
        "id": "nHGywIVPSlMo"
      },
      "outputs": [],
      "source": [
        "def split_data(data : bytes,split_length : int):\n",
        "  return [data[i:i+split_length] for i in range(0, len(data), split_length)]\n",
        "\n",
        "def to_trits(n):\n",
        "    if n == 0:\n",
        "        return [0]\n",
        "    digits = []\n",
        "    while n:\n",
        "        digits.append(int(n % 3))\n",
        "        n //= 3\n",
        "    return digits[::-1]\n",
        "\n",
        "def add_ecc(split_bytes,ecc_interval):\n",
        "  r = []\n",
        "  for b in split_bytes:\n",
        "    r.append(hamming_codec.encode(int.from_bytes(b,'big'),ecc_interval * 8))\n",
        "  return r\n",
        "\n",
        "def binaries_to_trits(binaries,max) : #バイナリの配列をtritの配列に\n",
        "  r = []\n",
        "  for b in binaries : \n",
        "    t = to_trits(int(b,base=2))\n",
        "    t = ([0] * (max - len(t))) + t\n",
        "    r.append(t)\n",
        "  return r\n",
        "\n",
        "\n",
        "def trits_to_binaries(trits,trit_max,bit_max) :\n",
        "  r = []\n",
        "  trits_blocks = [trits[idx:idx + trit_max] for idx in range(0,len(trits), trit_max)]\n",
        "  for b in trits_blocks :\n",
        "    maped = map(str, b)\n",
        "    bin_str = bin(int(\"\".join(maped),3))\n",
        "    if (len(bin_str) - 2) < bit_max :\n",
        "      zeros = bit_max - (len(bin_str) - 2)\n",
        "      bin_str = bin_str[:2] + ('0' * zeros) + bin_str[2:]\n",
        "    r.append(bin_str)\n",
        "  return r\n",
        "\n",
        "\n",
        "def binaries_to_identified_bytes(binaries,address_size):\n",
        "  address = bytes()\n",
        "  data = bytes()\n",
        "  address_count = 0\n",
        "  for i,b in enumerate(binaries) :\n",
        "    h = hex(int(hamming_codec.decode(int(b,2),len(b)),2))[2:]\n",
        "    if h == '0' :\n",
        "      address_count += 1\n",
        "      continue\n",
        "    if len(h) % 2 != 0 :\n",
        "      h = '0'+ h\n",
        "    if address_count < address_size :\n",
        "      address += bytes.fromhex(h)\n",
        "      address_count += len(bytes.fromhex(h))\n",
        "    else :\n",
        "      data += bytes.fromhex(h)\n",
        "      trimed_address = bytearray()\n",
        "      for a in address :\n",
        "        if a == bytes.fromhex('3f')[0] :\n",
        "          continue\n",
        "        trimed_address.append(a)\n",
        "  return (bytes(trimed_address),data)\n",
        "\n",
        "\n",
        "\n",
        "def trits_to_dna_bases(trits,origin_base = 'A') :\n",
        "  bases = [origin_base]\n",
        "  mapper = {\n",
        "      'A' : {0:'G',1:'C',2:'T'},\n",
        "      'C' : {0:'T',1:'G',2:'A'},\n",
        "      'G' : {0:'A',1:'T',2:'C'},\n",
        "      'T' : {0:'C',1:'A',2:'G'}\n",
        "  }\n",
        "  for i,t in enumerate(trits):\n",
        "    bases.append(mapper[bases[i]][t])\n",
        "  return bases\n",
        "\n",
        "def dna_bases_to_trits(bases) :\n",
        "  trits = []\n",
        "  mapper = {\n",
        "      'A' : {'G':0,'C':1,'T':2},\n",
        "      'C' : {'T':0,'G':1,'A':2},\n",
        "      'G' : {'A':0,'T':1,'C':2},\n",
        "      'T' : {'C':0,'A':1,'G':2}\n",
        "  }\n",
        "  for i,b in enumerate(bases):\n",
        "    if i == len(bases) - 1 : break\n",
        "    if bases[i + 1] == b : continue\n",
        "    trits.append(mapper[b][bases[i + 1]])\n",
        "  return trits\n",
        "\n",
        "def encode_binary_to_dna_bases(file_path,bytes_per_oligo,ecc_interval = 4,address_size = 4) :\n",
        "  trit_max = {1:8,2:14,3:19,4:24} #ecc_intervalごとのtritのmax桁数\n",
        "  bit_max = {1:12,2:21,3:29,4:38} #ecc_intervalごとのECC付与後bitのmax桁数\n",
        "  with open(file_path,'rb') as f:\n",
        "    data_bytes = f.read()#生データ\n",
        "    sd1 = split_data(data_bytes,bytes_per_oligo)#オリゴごとにデータを分ける\n",
        "\n",
        "    identified_datas = [] \n",
        "    for i,d in enumerate(sd1):#オリゴごとのデータにアドレス(hex文字列のbytes)を付与する\n",
        "      address = bytes(hex(i + 1)[2:],'utf-8')\n",
        "      address = bytes(('?'*(address_size - len(address))).encode('utf-8')) + address #アドレスサイズ合わせ\n",
        "      identified_datas.append((address,d))\n",
        "    \n",
        "    identified_and_spilt_datas = [] #ECCを付与するためにECC付与間隔ごとに各オリゴのデータとアドレスのbytesを分ける\n",
        "    for t in identified_datas :\n",
        "      identified_and_spilt_datas.append((split_data(t[0],ecc_interval),split_data(t[1],ecc_interval)))\n",
        "    \n",
        "    encoded_datas = [] #ECCを付与したデータ(2進数)\n",
        "    for t in identified_and_spilt_datas:\n",
        "      encoded_datas.append((add_ecc(t[0],ecc_interval),add_ecc(t[1],ecc_interval)))\n",
        "\n",
        "    encoded_datas_trits = [] #ECCを付与したデータ(Trit(3進数))\n",
        "    for t in encoded_datas :\n",
        "      encoded_datas_trits.append(\n",
        "        list(itertools.chain(*(binaries_to_trits(t[0],trit_max[ecc_interval]) + binaries_to_trits(t[1],trit_max[ecc_interval]))))\n",
        "        )\n",
        "    \n",
        "    target_bases = [] #合成すべき目標の配列にエンコード\n",
        "    for t in encoded_datas_trits :\n",
        "      target_bases.append(trits_to_dna_bases(t))\n",
        "\n",
        "  return target_bases\n",
        "\n",
        "\n",
        "def is_hex(val):\n",
        "    try:\n",
        "        int(val, 16)\n",
        "        return True\n",
        "    except ValueError:\n",
        "        return False\n",
        "\n",
        "def decode_dna_bases_to_binary(bases,ecc_interval = 4,address_size = 4) :\n",
        "  trit_max = {1:8,2:14,3:19,4:24} #ecc_intervalごとのtritのmax桁数\n",
        "  bit_max = {1:12,2:21,3:29,4:38} #ecc_intervalごとのECC付与後bitのmax桁数\n",
        "  ibs = []\n",
        "  address_error = []\n",
        "  for b in bases:\n",
        "    at = dna_bases_to_trits(b)\n",
        "    bs = trits_to_binaries(at,trit_max[ecc_interval],bit_max[ecc_interval])\n",
        "    #print(bs)\n",
        "    ib = binaries_to_identified_bytes(bs,address_size)\n",
        "    if is_hex(ib[0]) :\n",
        "      ibs.append(ib)\n",
        "    else :\n",
        "      address_error.append(ib)\n",
        "\n",
        "  ibs.sort(key = lambda x: int(x[0],16))\n",
        "  print('address error counts : ' + str(len(address_error)))\n",
        "  return ibs\n",
        "\n",
        "def decode_dna_bases_to_binary_with_details(bases,ecc_interval = 4,address_size = 4) :\n",
        "  trit_max = {1:8,2:14,3:19,4:24} #ecc_intervalごとのtritのmax桁数\n",
        "  bit_max = {1:12,2:21,3:29,4:38} #ecc_intervalごとのECC付与後bitのmax桁数\n",
        "  ibs = []\n",
        "  address_error = []\n",
        "  for b in bases:\n",
        "    at = dna_bases_to_trits(b)\n",
        "    bs = trits_to_binaries(at,trit_max[ecc_interval],bit_max[ecc_interval])\n",
        "    #print(bs)\n",
        "    ib = binaries_to_identified_bytes(bs,address_size)\n",
        "    if is_hex(ib[0]) :\n",
        "      ibs.append(ib)\n",
        "    else :\n",
        "      address_error.append(ib)\n",
        "\n",
        "  ibs.sort(key = lambda x: int(x[0],16))\n",
        "  #print('address error counts : ' + str(len(address_error)))\n",
        "  address_error_count = len(address_error)\n",
        "  return ibs,address_error_count\n",
        "\n",
        "def weighted_random(denominator:int,numerator:int):\n",
        "  n = list(range(numerator))\n",
        "  r = random.randint(0,denominator)\n",
        "  return r in n\n",
        "\n",
        "def weighted_random_exclusive(denominator:int,numerators:list):\n",
        "  ns = []\n",
        "  for i,n in enumerate(numerators) :\n",
        "    if i == 0 : \n",
        "      if n == 0 :\n",
        "        ns.append([-1])\n",
        "      else :\n",
        "        ns.append(list(range(n)))\n",
        "    elif n == 0 :\n",
        "       ns.append([-1])\n",
        "    else :\n",
        "      ns.append(list(range(max(ns[i - 1]) + 1,max(ns[i - 1]) + 1 + n)))\n",
        "  r = random.randint(0,denominator)\n",
        "  for i,n in enumerate(ns) :\n",
        "    if r in n :\n",
        "      return i\n",
        "  return -1\n",
        "  \n",
        "def extension(base : str, cycle : int,miss_extension_denominator : int,miss_extension_numerator : int,deletion_and_over_extension_denominator : int,deletion_numerator : int,over_extension_numerator : int) :\n",
        "  bases = ['A','G','C','T']\n",
        "  r = []\n",
        "  for i in range(cycle):\n",
        "    ext_base = base\n",
        "    if weighted_random(miss_extension_denominator,miss_extension_numerator):# 塩基ミス\n",
        "      other_bases = [s for s in bases if base not in s]\n",
        "      ext_base = other_bases[random.randint(0,2)]\n",
        "\n",
        "    ns = [over_extension_numerator,deletion_numerator]\n",
        "    deletion_or_over = weighted_random_exclusive(deletion_and_over_extension_denominator,ns)# 伸長失敗と過剰に伸長\n",
        "    if deletion_or_over == -1 :# 正常に伸長\n",
        "      r.append(ext_base)\n",
        "      continue\n",
        "    if deletion_or_over == 0 : #過剰に伸長\n",
        "      r.append(ext_base)\n",
        "      r.append(ext_base)\n",
        "      continue\n",
        "    if deletion_or_over == 1 : #欠損\n",
        "      continue\n",
        "\n",
        "  return r\n",
        "\n",
        "def synthesis_dna(bases : list,\n",
        "                  reaction_cycle : int,\n",
        "                  miss_extension_denominator : int,\n",
        "                  miss_extension_numerator : int,\n",
        "                  deletion_and_over_extension_denominator : int,\n",
        "                  deletion_numerator : int,\n",
        "                  over_extension_numerator : int) :\n",
        "  synthesis_result = []\n",
        "  for b in bases :\n",
        "    r = extension(b,reaction_cycle,miss_extension_denominator,miss_extension_numerator,deletion_and_over_extension_denominator,deletion_numerator,over_extension_numerator)\n",
        "    synthesis_result = synthesis_result + r\n",
        "  return synthesis_result\n",
        "\n",
        "def extension_with_details(base : str, cycle : int,miss_extension_denominator : int,miss_extension_numerator : int,deletion_and_over_extension_denominator : int,deletion_numerator : int,over_extension_numerator : int) :\n",
        "  bases = ['A','G','C','T']\n",
        "  r = []\n",
        "  miss_extension_count = 0\n",
        "  over_extension_count = 0\n",
        "  deletion_count = 0\n",
        "  for i in range(cycle):\n",
        "    ext_base = base\n",
        "    if weighted_random(miss_extension_denominator,miss_extension_numerator):# 塩基ミス\n",
        "      other_bases = [s for s in bases if base not in s]\n",
        "      ext_base = other_bases[random.randint(0,2)]\n",
        "      miss_extension_count += 1\n",
        "\n",
        "    ns = [over_extension_numerator,deletion_numerator]\n",
        "    deletion_or_over = weighted_random_exclusive(deletion_and_over_extension_denominator,ns)# 伸長失敗と過剰に伸長\n",
        "    if deletion_or_over == -1 :# 正常に伸長\n",
        "      r.append(ext_base)\n",
        "      continue\n",
        "    if deletion_or_over == 0 : #過剰に伸長\n",
        "      r.append(ext_base)\n",
        "      r.append(ext_base)\n",
        "      over_extension_count += 1\n",
        "      continue\n",
        "    if deletion_or_over == 1 : #欠損\n",
        "      deletion_count += 1\n",
        "      continue\n",
        "\n",
        "  return r,miss_extension_count,over_extension_count,deletion_count\n",
        "\n",
        "def synthesis_dna_with_details(bases : list,\n",
        "                  reaction_cycle : int,\n",
        "                  miss_extension_denominator : int,\n",
        "                  miss_extension_numerator : int,\n",
        "                  deletion_and_over_extension_denominator : int,\n",
        "                  deletion_numerator : int,\n",
        "                  over_extension_numerator : int) :\n",
        "  synthesis_result = []\n",
        "  miss_extension_count = over_extension_count = deletion_count = 0\n",
        "  for b in bases :\n",
        "    r,_miss_extension_count,_over_extension_count,_deletion_count = extension_with_details(b,reaction_cycle,miss_extension_denominator,miss_extension_numerator,deletion_and_over_extension_denominator,deletion_numerator,over_extension_numerator)\n",
        "    synthesis_result = synthesis_result + r\n",
        "    miss_extension_count += _miss_extension_count\n",
        "    over_extension_count += _over_extension_count\n",
        "    deletion_count += _deletion_count\n",
        "  return synthesis_result,miss_extension_count,over_extension_count,deletion_count\n",
        "\n",
        "def tuple_list_to_dict(tups):\n",
        "  dic = {}\n",
        "  for a, b in tups:\n",
        "      dic.setdefault(a, []).append(b)\n",
        "  return dic\n",
        "\n",
        "def extract_list_of_tuple(tups):\n",
        "    return [list(tup) for tup in zip(*tups)]\n",
        "\n",
        "def count_diff_byte(ref : bytes,sample :bytes):\n",
        "  diff_count = 0 if len(ref) == len(sample) else (len(ref) - len(sample))\n",
        "  if diff_count < 0 :\n",
        "    diff_count = abs(diff_count)\n",
        "    for i,r in enumerate(ref):\n",
        "      diff_count = diff_count if r == sample[i] else diff_count + 1\n",
        "  else :\n",
        "    for i,s in enumerate(sample):\n",
        "      diff_count = diff_count if s == ref[i] else diff_count + 1\n",
        "  return diff_count\n",
        "\n",
        "def get_file_size(file_path :str):\n",
        "  with open(file_path,'rb') as f:\n",
        "    return len(f.read())\n",
        "\n",
        "def flat(l :list):\n",
        "  return list(itertools.chain.from_iterable(l))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oi6ZFyuJSW1K"
      },
      "source": [
        "# 通しで実行"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SWyPRvBZx-_j"
      },
      "outputs": [],
      "source": [
        "######################\n",
        "#変換関連パラメーター#\n",
        "######################\n",
        "\n",
        "bytes_per_oligo = 16 #オリゴ当たりのデータ量(byte)\n",
        "ecc_interval = 4 #何byteごとにECCを付与するか Max=4\n",
        "address_size = 4 #アドレスのサイズ(byte)\n",
        "\n",
        "######################\n",
        "#合成関連パラメーター#\n",
        "######################\n",
        "deletion_and_over_extension_denominator = 1000 #欠損と過剰伸長の確率の分母\n",
        "deletion_numerator = 100 #欠損確率の分子\n",
        "over_extension_numerator = 0 #過剰伸長確率の分子\n",
        "miss_extension_denominator = 1000 #塩基ミスの確率の分母\n",
        "miss_extension_numerator = 0#塩基ミスの確率の分子\n",
        "reaction_cycle = 3 #伸長反応のリアクション数⇒目標伸長数\n",
        "\n",
        "\n",
        "#ファイルから目標の配列生成\n",
        "#オリゴごとの目標配列のリストが得られる\n",
        "targets = encode_binary_to_dna_bases('1k_data',bytes_per_oligo,ecc_interval,address_size)\n",
        "print(list(itertools.chain.from_iterable(targets)))\n",
        "#print('目標の配列')\n",
        "#print(targets)\n",
        "\n",
        "#オリゴごとに合成\n",
        "synthesized_bases = []\n",
        "for t in targets :\n",
        "  synthesized_bases.append(synthesis_dna(t,\n",
        "                            reaction_cycle,\n",
        "                            miss_extension_denominator,\n",
        "                            miss_extension_numerator,\n",
        "                            deletion_and_over_extension_denominator,\n",
        "                            deletion_numerator,\n",
        "                            over_extension_numerator))\n",
        "\n",
        "#print('合成された配列')\n",
        "#print(synthesized_bases)\n",
        "\n",
        "decoded_targets = decode_dna_bases_to_binary(targets,ecc_interval,address_size)\n",
        "decoded_synthesized_bases = decode_dna_bases_to_binary(synthesized_bases,ecc_interval,address_size)\n",
        "\n",
        "print('デコードした目標配列')\n",
        "#print(decoded_targets)\n",
        "print(extract_list_of_tuple(decoded_targets)[0])\n",
        "print('デコードした合成配列')\n",
        "print(decoded_synthesized_bases)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# シミュレーション"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 256,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "address error counts : 0\n",
            "error address : 29/6400 (catfish : 0)\n",
            "\n",
            "error bytes in correct address : 1090/102400\n",
            "\n",
            "miss extension : 228/2323200\n",
            "\n",
            "over extension : 232565/2323200\n",
            "\n",
            "deletion : 23359/2323200\n",
            "\n",
            "start dump results.\n",
            "complete dump results. filename : output_2022-09-23-19-19-16.json\n"
          ]
        }
      ],
      "source": [
        "##############################\n",
        "#シミュレーションパラメーター#\n",
        "##############################\n",
        "from itertools import cycle\n",
        "\n",
        "\n",
        "number_of_simulations = 100 #シミュレーション回数\n",
        "input_file_path = '1k_data' #使用するデータのパス\n",
        "output_file_prefix = 'output'\n",
        "output_file_directory = './dump_datas/'\n",
        "will_dump = False\n",
        "\n",
        "\n",
        "\n",
        "######################  \n",
        "#変換関連パラメーター#\n",
        "######################\n",
        "\n",
        "bytes_per_oligo = 16 #オリゴ当たりのデータ量(byte)\n",
        "ecc_interval = 4 #何byteごとにECCを付与するか Max=4\n",
        "address_size = 4 #アドレスのサイズ(byte)\n",
        "\n",
        "\n",
        "\n",
        "######################\n",
        "#合成関連パラメーター#\n",
        "######################\n",
        "deletion_and_over_extension_denominator = 1000 #欠損と過剰伸長の確率の分母\n",
        "deletion_numerator = 10 #欠損確率の分子\n",
        "over_extension_numerator = 100 #過剰伸長確率の分子\n",
        "miss_extension_denominator = 10000 #塩基ミスの確率の分母\n",
        "miss_extension_numerator = 1#塩基ミスの確率の分子\n",
        "reaction_cycle = 3 #伸長反応のリアクション数⇒目標伸長数\n",
        "\n",
        "\n",
        "miss_extension_count = over_extension_count = deletion_count = address_error_count = address_error_catfish_count = error_bytes_in_correct_address_data = 0\n",
        "\n",
        "\n",
        "errors_in_each_simulation = {'errors_in_each_target':[],'address_error_count':[],'error_bytes_in_correct_address_data':[],'address_error_catfish_count':[]}\n",
        "\n",
        "targets = encode_binary_to_dna_bases(input_file_path,bytes_per_oligo,ecc_interval,address_size)\n",
        "target_base_count = len(flat(targets))\n",
        "decoded_targets = decode_dna_bases_to_binary(targets,ecc_interval,address_size)\n",
        "decoded_targets_dict = tuple_list_to_dict(decoded_targets)\n",
        "available_address = extract_list_of_tuple(decoded_targets)[0]\n",
        "\n",
        "\n",
        "for i in range(number_of_simulations):\n",
        "    synthesized_bases = []\n",
        "    errors_in_each_target = {'miss_extension_count' : [],'over_extension_count' : [],'deletion_count':[]}\n",
        "    for t in targets :\n",
        "        r,_miss_extension_count,_over_extension_count,_deletion_count = synthesis_dna_with_details(t,reaction_cycle,miss_extension_denominator,miss_extension_numerator,deletion_and_over_extension_denominator,deletion_numerator,over_extension_numerator)\n",
        "        synthesized_bases.append(r)\n",
        "        miss_extension_count += _miss_extension_count\n",
        "        over_extension_count += _over_extension_count\n",
        "        deletion_count += _deletion_count\n",
        "        errors_in_each_target['miss_extension_count'].append(_miss_extension_count)\n",
        "        errors_in_each_target['over_extension_count'].append(_over_extension_count)\n",
        "        errors_in_each_target['deletion_count'].append(_deletion_count)\n",
        "        \n",
        "    decoded_synthesized_bases,_address_error_count = decode_dna_bases_to_binary_with_details(synthesized_bases,ecc_interval,address_size)\n",
        "    address_error_count += _address_error_count\n",
        "    _error_bytes_in_correct_address_data = 0\n",
        "    _address_error_catfish_count = 0\n",
        "    for d in decoded_synthesized_bases :\n",
        "        if d[0] in available_address :\n",
        "            _error_bytes_in_correct_address_data += count_diff_byte(decoded_targets_dict[d[0]][0],d[1])\n",
        "        else :\n",
        "            _address_error_catfish_count += 1\n",
        "    error_bytes_in_correct_address_data += _error_bytes_in_correct_address_data\n",
        "    address_error_catfish_count += _address_error_catfish_count\n",
        "    errors_in_each_simulation['errors_in_each_target'].append(errors_in_each_target)\n",
        "    errors_in_each_simulation['error_bytes_in_correct_address_data'].append(_error_bytes_in_correct_address_data)\n",
        "    errors_in_each_simulation['address_error_count'].append(_address_error_count)\n",
        "    errors_in_each_simulation['address_error_catfish_count'].append(_address_error_catfish_count)\n",
        "\n",
        "print('error address : ' + str(address_error_count + address_error_catfish_count) + '/' + str((len(targets) * number_of_simulations)) + ' (catfish : ' + str(address_error_catfish_count) + ')' +'\\n')\n",
        "print('error bytes in correct address : ' + str(error_bytes_in_correct_address_data) + '/' + str((get_file_size(input_file_path) * number_of_simulations))  +'\\n')\n",
        "print('miss extension : ' + str(miss_extension_count) + '/' + str(number_of_simulations * target_base_count * reaction_cycle)  +'\\n')\n",
        "print('over extension : ' + str(over_extension_count) + '/' + str(number_of_simulations * target_base_count * reaction_cycle)  +'\\n')\n",
        "print('deletion : ' + str(deletion_count) + '/' + str(number_of_simulations * target_base_count * reaction_cycle)  +'\\n')\n",
        "# print(errors_in_each_simulation)\n",
        "\n",
        "if will_dump :\n",
        "    sim_results_with_meta_data = {\n",
        "    'number_of_simulations':number_of_simulations,\n",
        "    'input_file_size':get_file_size(input_file_path),\n",
        "    'bytes_per_oligo':bytes_per_oligo,\n",
        "    'ecc_interval':ecc_interval,\n",
        "    'address_size':address_size,\n",
        "    'deletion_and_over_extension_denominator':deletion_and_over_extension_denominator,\n",
        "    'deletion_numerator':deletion_numerator,\n",
        "    'over_extension_numerator':over_extension_numerator,\n",
        "    'miss_extension_denominator':miss_extension_denominator,\n",
        "    'miss_extension_numerator':miss_extension_numerator,\n",
        "    'reaction_cycle':reaction_cycle,\n",
        "    'results' : errors_in_each_simulation,\n",
        "    }\n",
        "    json_str = json.dumps(sim_results_with_meta_data)\n",
        "    output_file_name = output_file_prefix + '_' + datetime.datetime.fromtimestamp(time.time()).strftime('%Y-%m-%d-%H-%M-%S') + '.json'\n",
        "    output_file_path = output_file_directory + output_file_name\n",
        "    print('start dump results.')\n",
        "    if not os.path.exists(output_file_directory):\n",
        "        os.makedirs(output_file_directory)\n",
        "    with open(output_file_path, mode='w') as f:\n",
        "        f.write(json_str)\n",
        "    print('complete dump results. filename : ' + output_file_name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1\n",
            "2\n"
          ]
        }
      ],
      "source": [
        "ref = bytes('abcdefg'.encode('utf-8'))\n",
        "sample = bytes('abcdef'.encode('utf-8'))\n",
        "print(count_diff_byte(ref,sample))\n",
        "ref = bytes('abcdef'.encode('utf-8'))\n",
        "sample = bytes('abcdefgh'.encode('utf-8'))\n",
        "print(count_diff_byte(ref,sample))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GqSpZ7lqSepf"
      },
      "source": [
        "# 以下メモ"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 223,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "99.9465\n"
          ]
        }
      ],
      "source": [
        "deno = 1000\n",
        "num = 10\n",
        "\n",
        "ts = 10000\n",
        "\n",
        "counts = 0\n",
        "for j in range(ts):\n",
        "    _counts = 0\n",
        "    for i in range(ts):\n",
        "        _counts = _counts + 1 if weighted_random(deno,num) else _counts\n",
        "    counts += _counts\n",
        "print(str(counts / ts))\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 253,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2022-09-23-19-02-44\n"
          ]
        }
      ],
      "source": [
        "print(datetime.datetime.fromtimestamp(time.time()).strftime('%Y-%m-%d-%H-%M-%S'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 206,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1024\n"
          ]
        }
      ],
      "source": [
        "print(get_file_size('1k_data'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 234,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1\n",
            "2\n",
            "0\n"
          ]
        }
      ],
      "source": [
        "ref = bytes('abcdefg'.encode('utf-8'))\n",
        "sample = bytes('abcdef'.encode('utf-8'))\n",
        "print(count_diff_byte(ref,sample))\n",
        "ref = bytes('abcdef'.encode('utf-8'))\n",
        "sample = bytes('abcdefgh'.encode('utf-8'))\n",
        "print(count_diff_byte(ref,sample))\n",
        "ref = bytes('abcdef'.encode('utf-8'))\n",
        "sample = bytes('abcdef'.encode('utf-8'))\n",
        "print(count_diff_byte(ref,sample))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-WuHJuNTYhiu",
        "outputId": "a0f35b7c-d956-4e4a-f403-9af632e9571e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "196\n"
          ]
        }
      ],
      "source": [
        "nums = [0,20,0]\n",
        "counts_of_one = 0\n",
        "for i in range(1000) :  \n",
        "  if weighted_random_exclusive(100,nums) == 1 :\n",
        "    counts_of_one += 1\n",
        "\n",
        "print(counts_of_one)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OC1q3Fc4y3u5"
      },
      "outputs": [],
      "source": [
        "\n",
        "bytes_per_oligo = 16 #オリゴ当たりのデータ量(byte)\n",
        "ecc_interval = 3 #何byteごとにECCを付与するか Max=4\n",
        "address_size = 4 #アドレスのサイズ(byte)\n",
        "\n",
        "trit_max = {1:8,\n",
        "            2:14,\n",
        "            3:19,\n",
        "            4:24}\n",
        "bit_max = {1:12,\n",
        "           2:21,\n",
        "           3:29,\n",
        "           4:38}\n",
        "print(trit_max)\n",
        "print(trit_max[ecc_interval])\n",
        "\n",
        "with open('long_data','rb') as f:\n",
        "  data_bytes = f.read()#生データ\n",
        "  sd1 = split_data(data_bytes,bytes_per_oligo)#オリゴごとにデータを分ける\n",
        "  identified_datas = [] #オリゴごとのデータにアドレス(hex文字列のbytes)を付与する\n",
        "  for i,d in enumerate(sd1):\n",
        "    address = bytes(hex(i + 1)[2:],'utf-8')\n",
        "    #zero = 0\n",
        "    #address = zero.to_bytes(address_size - len(address),'big') + address #アドレスサイズ合わせ\n",
        "    address = bytes(('?'*(address_size - len(address))).encode('utf-8')) + address #アドレスサイズ合わせ\n",
        "    identified_datas.append((address,d))\n",
        "  print(identified_datas)\n",
        "\n",
        "  identified_and_spilt_datas = [] #ECCを付与するためにECC付与間隔ごとに各オリゴのデータとアドレスのbytesを分ける\n",
        "  for t in identified_datas :\n",
        "    identified_and_spilt_datas.append((split_data(t[0],ecc_interval),split_data(t[1],ecc_interval)))\n",
        "  print(identified_and_spilt_datas)\n",
        "\n",
        "  encoded_datas = [] #ECCを付与したデータ(2進数)\n",
        "  for t in identified_and_spilt_datas:\n",
        "    encoded_datas.append((add_ecc(t[0],ecc_interval),add_ecc(t[1],ecc_interval)))\n",
        "  print(encoded_datas)\n",
        "\n",
        "  encoded_datas_trits = [] #ECCを付与したデータ(Trit(3進数))\n",
        "  for t in encoded_datas :\n",
        "    encoded_datas_trits.append(\n",
        "        list(itertools.chain(*(binaries_to_trits(t[0],trit_max[ecc_interval]) + binaries_to_trits(t[1],trit_max[ecc_interval]))))\n",
        "        )\n",
        "    #encoded_datas_trits.append((binaries_to_trits(t[0],trit_max[ecc_interval]),binaries_to_trits(t[1],trit_max[ecc_interval])))\n",
        "  \n",
        "  print(encoded_datas_trits)\n",
        "\n",
        "  bases = [] #合成すべき目標の配列にエンコード\n",
        "  for t in encoded_datas_trits :\n",
        "    bases.append(trits_to_dna_bases(t))\n",
        "\n",
        "  print(bases)\n",
        "\n",
        "\n",
        "  \n",
        "  bt = encoded_datas_trits[0]\n",
        "  bs = bases[0]\n",
        "  at = dna_bases_to_trits(bs)\n",
        "  print(at)\n",
        "  bs = trits_to_binaries(at,trit_max[ecc_interval],bit_max[ecc_interval])\n",
        "  #decoded = []\n",
        "  print(bs)\n",
        "  ib = binaries_to_identified_bytes(bs,address_size)\n",
        "  print('address:' + ib[0].decode('utf-8') + '\\ndata:' + ib[1].decode('utf-8'))\n",
        "  # print(bs)\n",
        "  # for b in bs :\n",
        "  #   h = hex(int(hamming_codec.decode(int(b,2),len(b)),2))[2:]\n",
        "  #   if h == '0' : continue\n",
        "  #   decoded.append(bytes.fromhex(h).decode('utf-8'))\n",
        "  # print(\"\".join(decoded))\n",
        "  # print(bt)\n",
        "  # print(bs)\n",
        "  # print(at)\n",
        "  # print(bt == at)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 242
        },
        "id": "DQUa4_lDGlZw",
        "outputId": "9961b3d4-1fbb-4f4d-eac7-e6c65d870598"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-a6fa1420bbe3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcycle\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m   \u001b[0mext_base\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbase\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m   \u001b[0;32mif\u001b[0m \u001b[0mweighted_random\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmiss_extension_denominator\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmiss_extension_numerator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;31m# 塩基ミス\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m     \u001b[0mother_bases\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0ms\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ms\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbases\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mbase\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0mext_base\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mother_bases\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'weighted_random' is not defined"
          ]
        }
      ],
      "source": [
        "deletion_and_over_extension_denominator = 1000\n",
        "deletion_numerator = 100\n",
        "\n",
        "#over_extension_denominator = 1000\n",
        "over_extension_numerator = 300\n",
        "\n",
        "miss_extension_denominator = 1000\n",
        "miss_extension_numerator = 1\n",
        "\n",
        "cycle = 2\n",
        "\n",
        "bases = ['A','G','C','T']\n",
        "base = 'A'\n",
        "\n",
        "output = []\n",
        "\n",
        "for i in range(cycle):\n",
        "  ext_base = base\n",
        "  if weighted_random(miss_extension_denominator,miss_extension_numerator):# 塩基ミス\n",
        "    other_bases = [s for s in bases if base not in s]\n",
        "    ext_base = other_bases[random.randint(0,2)]\n",
        "\n",
        "  ns = [over_extension_numerator,deletion_numerator]\n",
        "  deletion_or_over = weighted_random_exclusive(deletion_and_over_extension_denominator,ns)# 伸長失敗と過剰に伸長\n",
        "  print(deletion_or_over)\n",
        "  if deletion_or_over == -1 :# 正常に伸長\n",
        "    output.append(ext_base)\n",
        "    continue\n",
        "  if deletion_or_over == 0 : #過剰に伸長\n",
        "    output.append(ext_base)\n",
        "    output.append(ext_base)\n",
        "    continue\n",
        "  if deletion_or_over == 1 : #欠損\n",
        "    continue\n",
        "print(output)\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.10.6 ('tdtsim')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.6"
    },
    "vscode": {
      "interpreter": {
        "hash": "f7189aa3fc29389988f4e2fceae270be99577ea5c7379080c58094f7f1a3853c"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
